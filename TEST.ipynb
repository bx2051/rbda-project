{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ede70e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import col, when, count, lit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCSFilesRead\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# file path \n",
    "file_location = \"/home/bx2051/glucose.csv\"\n",
    "\n",
    "def read_csv_with_inferred_schema(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with inferred schema.\n",
    "    transfer string data type to factor\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: The DataFrame with inferred schema.\n",
    "    \"\"\"\n",
    "    # Read the CSV file with inferred schema\n",
    "    data = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "df = read_csv_with_inferred_schema(file_location)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c54ab3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_df = df.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "82926e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_logistic_regression_model(data, max_iter=100, reg_param=0.02, elastic_net_param=0.8):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression model using the given dataset, feature columns, and label column.\n",
    "\n",
    "    Args:\n",
    "        data (pyspark.sql.DataFrame): The input DataFrame containing the features and label.\n",
    "        max_iter (int): Maximum number of iterations (default: 100).\n",
    "        reg_param (float): Regularization parameter (default: 0.02).\n",
    "        elastic_net_param (float): ElasticNet mixing parameter, in range [0, 1] (default: 0.8).\n",
    "\n",
    "    Returns:\n",
    "        pyspark.ml.classification.LogisticRegressionModel: The trained logistic regression model.\n",
    "    \"\"\"\n",
    "    train_model_dataset = data.withColumn(\"diabet\",\n",
    "                                          when(data[\"glucose\"] > 6.1, 1).otherwise(0))\n",
    "\n",
    "    # Drop the original 'glucose' column if you don't need it anymore\n",
    "    train_model_dataset = train_model_dataset.drop(\"glucose\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=train_model_dataset.columns[:-1], outputCol=\"features\")\n",
    "    train_model_dataset = assembler.transform(train_model_dataset)\n",
    "\n",
    "    tri = LogisticRegression(maxIter=max_iter,\n",
    "                             regParam=reg_param,\n",
    "                             elasticNetParam=elastic_net_param,\n",
    "                             featuresCol=\"features\",\n",
    "                             labelCol=\"diabet\")\n",
    "\n",
    "    lr_model = tri.fit(train_model_dataset)\n",
    "\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2d9b0090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Set the seed and seperate the train and test data \n",
    "seed = 5043\n",
    "trainingData, testData = limited_df.randomSplit([0.7, 0.3], seed=seed)\n",
    "\n",
    "\n",
    "# # Assuming your DataFrame is named 'test_model_dataset'\n",
    "# # Add a new column 'glucose_binary' based on the condition\n",
    "# test_model_dataset = trainingData.withColumn(\"diabet\", \n",
    "#                                                    when(trainingData[\"glucose\"] > 6.1, 1).otherwise(0))\n",
    "\n",
    "# # Drop the original 'glucose' column if you don't need it anymore\n",
    "# test_model_dataset = test_model_dataset.drop(\"glucose\")\n",
    "\n",
    "# # Print the first few rows to verify the change\n",
    "# # test_model_dataset.show()\n",
    "\n",
    "# # test_model_dataset.printSchema()\n",
    "# assembler = VectorAssembler(inputCols= test_model_dataset.columns, outputCol=\"features\")\n",
    "# test_model_dataset = assembler.transform(test_model_dataset)\n",
    "\n",
    "# tri=LogisticRegression(maxIter=10,\n",
    "#                        regParam=0.01,\n",
    "#                        featuresCol=\"features\",\n",
    "#                        labelCol=\"diabet\")\n",
    "# lr_model = tri.fit(test_model_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "913bb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the train model \n",
    "lr_model = train_logistic_regression_model(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb60921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic_regression_result(model, testData):\n",
    "    \"\"\"\n",
    "    Makes predictions using the trained model on the testing data.\n",
    "\n",
    "    Args:\n",
    "        model (pyspark.ml.classification.LogisticRegressionModel): The trained logistic regression model.\n",
    "        test_data (pyspark.sql.DataFrame): The testing data.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: The DataFrame with predicted results and actual labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    testData = testData.withColumn(\"diabet\", \n",
    "                                            when(testData[\"glucose\"] > 6.1, 1).otherwise(0))\n",
    "\n",
    "    testData = testData.drop(\"glucose\")\n",
    "    \n",
    "    # Check if 'features' column exists and drop it\n",
    "    if 'features' in testData.columns:\n",
    "        testData = testData.drop('features')\n",
    "\n",
    "    # Assuming 'actual_outcome' is the column in the original data used as the label\n",
    "    # Make sure it's included when transforming testData\n",
    "    testData = testData.withColumn(\"label\", testData[\"diabet\"])\n",
    "    testData = testData.drop(\"diabet\")\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[col for col in testData.columns if col != \"label\"], outputCol=\"features\")\n",
    "    testData = assembler.transform(testData)\n",
    "\n",
    "        # Make predictions using the trained model\n",
    "    predictions = model.transform(testData)\n",
    "    predictions = predictions.select(\"prediction\", \"label\")\n",
    "    # Transfer the prediction double type to integer type \n",
    "    predictions = predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"integer\"))\n",
    "    # Example\n",
    "    # ['prediction',  'label']\n",
    "       \n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "984f2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testData= testData.drop(\"glucose\")\n",
    "\n",
    "testData.columns\n",
    "testData = testData.withColumn(\"diabet\", \n",
    "                                                   when(testData[\"glucose\"] > 6.1, 1).otherwise(0))\n",
    "testData = testData.drop(\"glucose\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb4687a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'features' column exists and drop it\n",
    "if 'features' in testData.columns:\n",
    "    testData = testData.drop('features')\n",
    "\n",
    "# Assuming 'actual_outcome' is the column in the original data used as the label\n",
    "# Make sure it's included when transforming testData\n",
    "testData = testData.withColumn(\"label\", testData[\"diabet\"])\n",
    "testData = testData.drop(\"diabet\")\n",
    "    \n",
    "assembler = VectorAssembler(inputCols=[col for col in testData.columns if col != \"label\"], outputCol=\"features\")\n",
    "testData = assembler.transform(testData)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "predictions = lr_model.transform(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c6d6674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    1|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"glucose\" among (age, fy_diastolic_blood_pressure_low, fy_systolic_blood_pressure_high, fy_hemameba, fy_neutrophilic_granulocyte_percentage, fy_percentage_of_lymphocytes, fy_percentage_of_monocytes, fy_absolute_neutrophil_count, fy_lymphocyte_number, fy_absolute_value_of_monocytes, fy_absolute_eosinophil_count, fy_absolute_value_of_basophils, fy_erythrocyte, fy_hemoglobin, fy_mean_corpuscular_volume, fy_mean_corpuscular_hemoglobin, fy_mean_corpuscular_hemoglobin_concentration, fy_red_blood_cell_distribution_width_CV, fy_red_blood_cell_distribution_width_SD, fy_blood_platelet, fy_platelet_distribution_width, fy_mean_platelet_volume, fy_large_platelet_ratio, fy_pct, fy_whole_blood_viscosity_values_1pas_the_shear_rate_200s, fy_whole_blood_viscosity_values_2Pas_shear_rate30s, fy_whole_blood_viscosity_values_3Pas_shear_rate5s, fy_whole_blood_viscosity_values_4Pas_shear_rate1s, fy_ESR_blood_sedimentation, fy_hematocrit56, fy_red_cell_assembling_index, fy_the_K_value_of_the_ESR_equation, fy_glutamic_pyruvic_transaminase, fy_glutamic_oxalacetic_transaminase, fy_total_protein, fy_albumin, fy_total_bilirubin, fy_direct_bilirubin, fy_alkaline_phosphatase, fy_r_glutamyl_transpeptidase, fy_glucose, fy_carbamide, fy_creatinine, fy_trioxypurine, fy_total_cholesterol, fy_triglyceride, fy_high_density_lipoprotein_cholesterol, fy_low_density_lipoprotein_cholesterin, fy_creatine_phosphate_kinase, fy_sreatine_kinase_isoenzyme, fy_lactic_dehydrogenase, fy_a_hydroxybutyrate_dehydrogenase, fy_calcium, sy_phosphorus, fy_millet_grass_millet_c, fy_globulin, fy_albumin_globulin, fy_high_density_low_density, fy_apolipoprotein_ai, fy_apolipoprotein_B100, fy_carcino_embryonic_antigen, fy_alpha_fetoprotein, fy_free_prostate_specific_antigen, fy_total_prostate_specific_antigen, fy_proportion, fy_ph_value, fy_thyroid_stimulating_hormone, fy_free_T3, fy_free_T4, fy_500_left, fy_1000_left, fy_2000_left, fy_3000_left, fy_4000_left, fy_6000_left, fy_500_right, fy_1000_right, fy_2000_right, fy_3000_right, fy_4000_right, fy_6000_right, sy_diastolic_blood_pressure_low, sy_systolic_blood_pressure_high, sy_hemameba, sy_neutrophilic_granulocyte_percentage, sy_percentage_of_lymphocytes, sy_absolute_neutrophil_count, sy_lymphocyte_number, sy_absolute_value_of_monocytes, sy_absolute_eosinophil_count, sy_absolute_value_of_basophils, sy_erythrocyte, sy_hemoglobin, sy_mean_corpuscular_volume, sy_mean_corpuscular_hemoglobin, sy_mean_corpuscular_hemoglobin_concentration, sy_red_blood_cell_distribution_width_CV, sy_red_blood_cell_distribution_width_SD, sy_blood_platelet, sy_platelet_distribution_width, sy_mean_platelet_volume, sy_large_platelet_ratio, sy_pct, sy_whole_blood_viscosity_values_1pas_the_shear_rate_200s, sy_whole_blood_viscosity_values_2Pas_shear_rate30s, sy_whole_blood_viscosity_values_3Pas_shear_rate5s, sy_whole_blood_viscosity_values_4Pas_shear_rate1s, sy_ESR_blood_sedimentation, sy_hematocrit56, sy_red_cell_assembling_index, sy_the_K_value_of_the_ESR_equation, sy_glutamic_pyruvic_transaminase, sy_glutamic_oxalacetic_transaminase, sy_total_protein, sy_albumin, sy_total_bilirubin, sy_direct_bilirubin, sy_alkaline_phosphatase, sy_r_glutamyl_transpeptidase, sy_glucose, sy_carbamide, sy_creatinine, sy_trioxypurine, sy_total_cholesterol, sy_triglyceride, sy_high_density_lipoprotein_cholesterol, sy_low_density_lipoprotein_cholesterin, sy_creatine_phosphate_kinase, sy_sreatine_kinase_isoenzyme, sy_lactic_dehydrogenase, sy_a_hydroxybutyrate_dehydrogenase, sy_calcium, sy_millet_grass_millet_c, sy_globulin, sy_albumin_globulin, sy_high_density_low_density, sy_apolipoprotein_ai, sy_apolipoprotein_B100, sy_carcino_embryonic_antigen, sy_alpha_fetoprotein, sy_free_prostate_specific_antigen, sy_total_prostate_specific_antigen, sy_proportion, sy_ph_value, sy_thyroid_stimulating_hormone, sy_free_T3, sy_free_T4, sy_500_left, sy_1000_left, sy_2000_left, sy_3000_left, sy_4000_left, sy_6000_left, sy_500_right, sy_1000_right, sy_2000_right, sy_3000_right, sy_4000_right, sy_6000_right, label, features)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/state/partition1/job-46498915/ipykernel_1740288/4183939094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpredict_logistic_regression_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/state/partition1/job-46498915/ipykernel_1740288/2512529941.py\u001b[0m in \u001b[0;36mpredict_logistic_regression_result\u001b[0;34m(model, testData)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     testData = testData.withColumn(\"diabet\", \n\u001b[0;32m---> 14\u001b[0;31m                                             when(testData[\"glucose\"] > 6.1, 1).otherwise(0))\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtestData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glucose\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \"\"\"\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"glucose\" among (age, fy_diastolic_blood_pressure_low, fy_systolic_blood_pressure_high, fy_hemameba, fy_neutrophilic_granulocyte_percentage, fy_percentage_of_lymphocytes, fy_percentage_of_monocytes, fy_absolute_neutrophil_count, fy_lymphocyte_number, fy_absolute_value_of_monocytes, fy_absolute_eosinophil_count, fy_absolute_value_of_basophils, fy_erythrocyte, fy_hemoglobin, fy_mean_corpuscular_volume, fy_mean_corpuscular_hemoglobin, fy_mean_corpuscular_hemoglobin_concentration, fy_red_blood_cell_distribution_width_CV, fy_red_blood_cell_distribution_width_SD, fy_blood_platelet, fy_platelet_distribution_width, fy_mean_platelet_volume, fy_large_platelet_ratio, fy_pct, fy_whole_blood_viscosity_values_1pas_the_shear_rate_200s, fy_whole_blood_viscosity_values_2Pas_shear_rate30s, fy_whole_blood_viscosity_values_3Pas_shear_rate5s, fy_whole_blood_viscosity_values_4Pas_shear_rate1s, fy_ESR_blood_sedimentation, fy_hematocrit56, fy_red_cell_assembling_index, fy_the_K_value_of_the_ESR_equation, fy_glutamic_pyruvic_transaminase, fy_glutamic_oxalacetic_transaminase, fy_total_protein, fy_albumin, fy_total_bilirubin, fy_direct_bilirubin, fy_alkaline_phosphatase, fy_r_glutamyl_transpeptidase, fy_glucose, fy_carbamide, fy_creatinine, fy_trioxypurine, fy_total_cholesterol, fy_triglyceride, fy_high_density_lipoprotein_cholesterol, fy_low_density_lipoprotein_cholesterin, fy_creatine_phosphate_kinase, fy_sreatine_kinase_isoenzyme, fy_lactic_dehydrogenase, fy_a_hydroxybutyrate_dehydrogenase, fy_calcium, sy_phosphorus, fy_millet_grass_millet_c, fy_globulin, fy_albumin_globulin, fy_high_density_low_density, fy_apolipoprotein_ai, fy_apolipoprotein_B100, fy_carcino_embryonic_antigen, fy_alpha_fetoprotein, fy_free_prostate_specific_antigen, fy_total_prostate_specific_antigen, fy_proportion, fy_ph_value, fy_thyroid_stimulating_hormone, fy_free_T3, fy_free_T4, fy_500_left, fy_1000_left, fy_2000_left, fy_3000_left, fy_4000_left, fy_6000_left, fy_500_right, fy_1000_right, fy_2000_right, fy_3000_right, fy_4000_right, fy_6000_right, sy_diastolic_blood_pressure_low, sy_systolic_blood_pressure_high, sy_hemameba, sy_neutrophilic_granulocyte_percentage, sy_percentage_of_lymphocytes, sy_absolute_neutrophil_count, sy_lymphocyte_number, sy_absolute_value_of_monocytes, sy_absolute_eosinophil_count, sy_absolute_value_of_basophils, sy_erythrocyte, sy_hemoglobin, sy_mean_corpuscular_volume, sy_mean_corpuscular_hemoglobin, sy_mean_corpuscular_hemoglobin_concentration, sy_red_blood_cell_distribution_width_CV, sy_red_blood_cell_distribution_width_SD, sy_blood_platelet, sy_platelet_distribution_width, sy_mean_platelet_volume, sy_large_platelet_ratio, sy_pct, sy_whole_blood_viscosity_values_1pas_the_shear_rate_200s, sy_whole_blood_viscosity_values_2Pas_shear_rate30s, sy_whole_blood_viscosity_values_3Pas_shear_rate5s, sy_whole_blood_viscosity_values_4Pas_shear_rate1s, sy_ESR_blood_sedimentation, sy_hematocrit56, sy_red_cell_assembling_index, sy_the_K_value_of_the_ESR_equation, sy_glutamic_pyruvic_transaminase, sy_glutamic_oxalacetic_transaminase, sy_total_protein, sy_albumin, sy_total_bilirubin, sy_direct_bilirubin, sy_alkaline_phosphatase, sy_r_glutamyl_transpeptidase, sy_glucose, sy_carbamide, sy_creatinine, sy_trioxypurine, sy_total_cholesterol, sy_triglyceride, sy_high_density_lipoprotein_cholesterol, sy_low_density_lipoprotein_cholesterin, sy_creatine_phosphate_kinase, sy_sreatine_kinase_isoenzyme, sy_lactic_dehydrogenase, sy_a_hydroxybutyrate_dehydrogenase, sy_calcium, sy_millet_grass_millet_c, sy_globulin, sy_albumin_globulin, sy_high_density_low_density, sy_apolipoprotein_ai, sy_apolipoprotein_B100, sy_carcino_embryonic_antigen, sy_alpha_fetoprotein, sy_free_prostate_specific_antigen, sy_total_prostate_specific_antigen, sy_proportion, sy_ph_value, sy_thyroid_stimulating_hormone, sy_free_T3, sy_free_T4, sy_500_left, sy_1000_left, sy_2000_left, sy_3000_left, sy_4000_left, sy_6000_left, sy_500_right, sy_1000_right, sy_2000_right, sy_3000_right, sy_4000_right, sy_6000_right, label, features)"
     ]
    }
   ],
   "source": [
    "# predictions = predictions.select(\"prediction\", \"label\")\n",
    "# # Transfer the prediction double type to integer type \n",
    "# predictions = predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"integer\"))\n",
    "\n",
    "\n",
    "# predictions= predict_logistic_regression_result(lr_model, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bbaba2d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         1|    1|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "|         0|    0|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57d5287c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "def calculate_sensitivity_specificity(predictions):\n",
    "    \"\"\"\n",
    "    Calculate sensitivity and specificity from a DataFrame containing labels and predictions.\n",
    "    \n",
    "    Args:\n",
    "    predictions (DataFrame): A DataFrame with two columns: 'label' and 'prediction'.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Returns a tuple containing sensitivity (True Positive Rate) and specificity (True Negative Rate).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate confusion matrix components\n",
    "    metrics = predictions.withColumn(\"TP\", when((col(\"label\") == 1) & (col(\"prediction\") == 1), 1).otherwise(0)) \\\n",
    "                          .withColumn(\"FN\", when((col(\"label\") == 1) & (col(\"prediction\") == 0), 1).otherwise(0)) \\\n",
    "                          .withColumn(\"FP\", when((col(\"label\") == 0) & (col(\"prediction\") == 1), 1).otherwise(0)) \\\n",
    "                          .withColumn(\"TN\", when((col(\"label\") == 0) & (col(\"prediction\") == 0), 1).otherwise(0)) \\\n",
    "                          .agg(\n",
    "                              count(when(col(\"TP\") == 1, True)).alias(\"TP\"),\n",
    "                              count(when(col(\"FN\") == 1, True)).alias(\"FN\"),\n",
    "                              count(when(col(\"FP\") == 1, True)).alias(\"FP\"),\n",
    "                              count(when(col(\"TN\") == 1, True)).alias(\"TN\")\n",
    "                          ).collect()[0]\n",
    "    \n",
    "    TP, FN, FP, TN = metrics[\"TP\"], metrics[\"FN\"], metrics[\"FP\"], metrics[\"TN\"]\n",
    "\n",
    "    # Calculate sensitivity (True Positive Rate)\n",
    "    TPR = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    # Calculate specificity (True Negative Rate)\n",
    "    TNR = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(\"\\t\\tPredicted Positive\\tPredicted Negative\\tSensitivity\")\n",
    "    print(\"Actual Positive\\t\", TP, \"\\t\\t\\t\\t\", FN, \"\\t\\t\", f\"{TPR:.3f}\")\n",
    "    print(\"Actual Negative\\t\", FP, \"\\t\\t\\t\\t\", TN, \"\\t\\t\", f\"{TNR:.3f}\")    \n",
    "    \n",
    "    return TPR, TNR\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming 'predictions' is a DataFrame with the necessary columns\n",
    "# TPR, TNR = calculate_sensitivity_specificity(predictions)\n",
    "\n",
    "\n",
    "# # Example Usage:\n",
    "# TPR, TNR = calculate_sensitivity_specificity(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04a8fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trainning logistic model with validation \n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "def train_with_train_validation_split(data):\n",
    "    \n",
    "    \n",
    "    #trainfer the glucose to binary 1 or 0 based on 6.1 and rename to \n",
    "    #dia bet \n",
    "    train_model_dataset = data.withColumn(\"diabet\",\n",
    "                                          when(data[\"glucose\"] > 6.1, 1).otherwise(0))\n",
    "\n",
    "    # Drop the original 'glucose' column if you don't need it anymore\n",
    "    train_model_dataset = train_model_dataset.drop(\"glucose\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=train_model_dataset.columns[:-1], outputCol=\"features\")\n",
    "    train_model_dataset = assembler.transform(train_model_dataset)\n",
    "\n",
    "    # Create an instance of the LogisticRegression estimator\n",
    "    undefined_model = LogisticRegression(featuresCol=\"features\", labelCol=\"diabet\")\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(undefined_model.maxIter, [10, 50, 100]) \\\n",
    "        .addGrid(undefined_model.regParam, [0, 0.01, 0.05, 0.1, 0.5, 1]) \\\n",
    "        .addGrid(undefined_model.elasticNetParam, [0.0, 0.1, 0.5, 0.8, 1]) \\\n",
    "        .build()\n",
    "\n",
    "    # Create a binary classification evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"diabet\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    # Create a TrainValidationSplit\n",
    "    tvs = TrainValidationSplit(estimator=undefined_model, estimatorParamMaps=param_grid, evaluator=evaluator, trainRatio=0.667)\n",
    "\n",
    "    # Fit the TrainValidationSplit to the training data\n",
    "    tvs_model = tvs.fit(train_model_dataset)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = tvs_model.bestModel\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3deb6cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 04:57:24 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/05/15 04:57:25 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/05/15 04:57:25 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "# limited_df = df.limit(100)\n",
    "# model = train_with_train_validation_split(limited_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24d5c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 5043\n",
    "# trainingData, testData = limited_df.randomSplit([0.7, 0.3], seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18e4baf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\t\tPredicted Positive\tPredicted Negative\tSensitivity\n",
      "Actual Positive\t 1 \t\t\t\t 0 \t\t 1.000\n",
      "Actual Negative\t 0 \t\t\t\t 37 \t\t 1.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# predictions= predict_logistic_regression_result(model, testData)\n",
    "# # Example Usage:\n",
    "# TPR, TNR = calculate_sensitivity_specificity(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9601b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "def combine_predictions(prediction1, prediction2, dataset_number):\n",
    "    \"\"\"\n",
    "    Combines predictions from two models based on the corresponding dataset for each data point.\n",
    "\n",
    "    Args:\n",
    "        prediction1 (pyspark.sql.DataFrame): The predictions from the first model, with columns \"prediction\" and \"label\".\n",
    "        prediction2 (pyspark.sql.DataFrame): The predictions from the second model, with columns \"prediction\" and \"label\".\n",
    "        dataset_number (pyspark.sql.DataFrame): The DataFrame indicating the dataset for each data point.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A DataFrame with the combined predictions, with columns \"prediction\" and \"label\".\n",
    "    \"\"\"\n",
    "    # Rename the columns in prediction1 and prediction2\n",
    "    prediction1 = prediction1.withColumnRenamed(\"prediction\", \"prediction_1\").withColumnRenamed(\"label\", \"label_1\")\n",
    "    prediction2 = prediction2.withColumnRenamed(\"prediction\", \"prediction_2\").withColumnRenamed(\"label\", \"label_2\")\n",
    "\n",
    "    # Combine the predictions and dataset_number DataFrames\n",
    "    combined_predictions = prediction1.join(prediction2, on=\"label_1\", how=\"inner\") \\\n",
    "                                      .join(dataset_number, on=\"label_1\", how=\"inner\")\n",
    "\n",
    "    # Create a new column 'combined_prediction' based on the comparison of predictions\n",
    "    combined_predictions = combined_predictions.withColumn(\n",
    "        \"combined_prediction\",\n",
    "        when(col(\"prediction_1\") == col(\"prediction_2\"), col(\"prediction_1\"))\n",
    "        .otherwise(\n",
    "            when(col(\"dataset\") == \"dataset1\", col(\"prediction_1\"))\n",
    "            .otherwise(col(\"prediction_2\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Select the required columns and rename them\n",
    "    combined_predictions = combined_predictions.select(\n",
    "        col(\"combined_prediction\").alias(\"prediction\"),\n",
    "        col(\"label_1\").alias(\"label\")\n",
    "    )\n",
    "\n",
    "    return combined_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffae9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def combine_predictions(prediction1, prediction2, dataset_number):\n",
    "    from pyspark.sql.functions import row_number\n",
    "    # Create a window specification without partitioning\n",
    "    windowSpec = Window.orderBy(lit('A'))  # 'A' is a dummy value to ensure a full scan in order\n",
    "\n",
    "    # Add a row index to each DataFrame that aligns across both DataFrames\n",
    "    prediction1 = prediction1.withColumn(\"id\", row_number().over(windowSpec) - 1)\n",
    "    prediction2 = prediction2.withColumn(\"id\", row_number().over(windowSpec) - 1)\n",
    "\n",
    "    # Ensure the columns are appropriately named to avoid conflicts and confusion\n",
    "    prediction1 = prediction1.withColumnRenamed(\"prediction\", \"prediction1\").drop(\"label\")\n",
    "    prediction2 = prediction2.withColumnRenamed(\"prediction\", \"prediction2\")\n",
    "\n",
    "    # Convert dataset_number list to DataFrame\n",
    "    ids = spark.createDataFrame([(i, d) for i, d in enumerate(dataset_number)], schema=\"id int, dataset string\")\n",
    "\n",
    "    # Join predictions with the dataset_number DataFrame using the index\n",
    "    combined_predictions = prediction1.join(prediction2, \"id\", \"inner\").join(ids, \"id\", \"inner\")\n",
    "\n",
    "    # Select predictions based on the dataset\n",
    "    combined_predictions = combined_predictions.withColumn(\n",
    "        \"combined_prediction\",\n",
    "        when(col(\"dataset\") == \"dataset1\", col(\"prediction1\"))\n",
    "        .otherwise(col(\"prediction2\"))\n",
    "    )\n",
    "\n",
    "    # Select the required columns and rename them\n",
    "    combined_predictions = combined_predictions.select(\n",
    "        col(\"combined_prediction\").alias(\"prediction\"),\n",
    "        \"label\"  # Choose label1 or label as needed, assuming label alignment or resolve as needed\n",
    "    )\n",
    "\n",
    "    return combined_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be7ed625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|         1|    1|\n",
      "|         0|    1|\n",
      "|         1|    0|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|         1|    1|\n",
      "|         1|    1|\n",
      "|         0|    0|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #Tesing \n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import lit\n",
    "# # Initialize Spark Session\n",
    "# spark = SparkSession.builder.appName(\"Combine Predictions Test\").getOrCreate()\n",
    "\n",
    "# # Sample data for prediction1\n",
    "# data1 = [(1, 1), (0, 1), (1, 0)]\n",
    "# prediction1 = spark.createDataFrame(data1, [\"prediction\", \"label\"])\n",
    "\n",
    "# # Sample data for prediction2\n",
    "# data2 = [(1, 1), (1, 1), (0, 0)]\n",
    "# prediction2 = spark.createDataFrame(data2, [\"prediction\", \"label\"])\n",
    "\n",
    "# # Sample dataset_number as a list\n",
    "# dataset_number = [\"dataset1\", \"dataset2\", \"dataset1\"]\n",
    "\n",
    "# # Display DataFrames\n",
    "# prediction1.show()\n",
    "# prediction2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9adb49e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 06:13:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/15 06:13:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|         1|    1|\n",
      "|         1|    0|\n",
      "|         1|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Example usage:\n",
    "# # spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "# result_df = combine_predictions(prediction1, prediction2, dataset_number)\n",
    "# result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d545dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
