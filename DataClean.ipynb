{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72753ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a508f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkStandaloneTest\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCSFilesRead\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# file path \n",
    "file_location = \"/home/bx2051/glucose.csv\"\n",
    "\n",
    "def read_csv_with_inferred_schema(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with inferred schema.\n",
    "    transfer string data type to factor\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: The DataFrame with inferred schema.\n",
    "    \"\"\"\n",
    "    # Read the CSV file with inferred schema\n",
    "    data = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "df = read_csv_with_inferred_schema(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e6c6a-7737-4db7-a77f-27bec6e970f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min, max, when\n",
    "\n",
    "def normalize_columns(data):\n",
    "    \"\"\"\n",
    "    Normalizes all columns in the DataFrame except for \"glucose\" and \"sy_glucose\".\n",
    "\n",
    "    Args:\n",
    "        data (pyspark.sql.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: The DataFrame with normalized columns.\n",
    "    \"\"\"\n",
    "    # Get the list of columns to normalize (excluding \"glucose\" and \"sy_glucose\")\n",
    "    columns_to_normalize = [col_name for col_name in data.columns if col_name not in [\"glucose\", \"sy_glucose\"]]\n",
    "\n",
    "    # Compute the minimum and maximum values for each column\n",
    "    min_max_dict = {}\n",
    "    for col_name in columns_to_normalize:\n",
    "        min_value = data.agg(min(col_name)).collect()[0][0]\n",
    "        max_value = data.agg(max(col_name)).collect()[0][0]\n",
    "        min_max_dict[col_name] = (min_value, max_value)\n",
    "\n",
    "    # Normalize each column\n",
    "    for col_name, (min_value, max_value) in min_max_dict.items():\n",
    "        if min_value != max_value:  # Avoid division by zero\n",
    "            data = data.withColumn(col_name, (col(col_name) - min_value) / (max_value - min_value))\n",
    "        else:\n",
    "            data = data.withColumn(col_name, col(col_name) - min_value)\n",
    "\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
