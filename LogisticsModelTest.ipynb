{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression_model(data, feature_columns, label_column, max_iter=100, reg_param=0.02, elastic_net_param=0.8):\n",
    "    # Create a VectorAssembler to combine feature columns into a single vector column\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    feature_data = assembler.transform(data)\n",
    "\n",
    "    # Assuming the label column might be categorical, use StringIndexer\n",
    "    indexer = StringIndexer(inputCol=label_column, outputCol=\"label\")\n",
    "    indexed_data = indexer.fit(feature_data).transform(feature_data)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = indexed_data.randomSplit([0.8, 0.2], seed=5043)\n",
    "\n",
    "    # Create a LogisticRegression estimator with the specified hyperparameters\n",
    "    lr = LogisticRegression(maxIter=max_iter, regParam=reg_param, elasticNetParam=elastic_net_param,\n",
    "                            featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "    # Train the logistic regression model\n",
    "    lr_model = lr.fit(train_data)\n",
    "    return lr_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45ed20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/05/14 08:49:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/14 08:49:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+------+\n",
      "|           score1|           score2|result|\n",
      "+-----------------+-----------------+------+\n",
      "|34.62365962451697| 78.0246928153624|     0|\n",
      "|30.28671076822607|43.89499752400101|     0|\n",
      "|35.84740876993872|72.90219802708364|     0|\n",
      "|60.18259938620976|86.30855209546826|     1|\n",
      "| 79.0327360507101| 75.3443764369103|     1|\n",
      "|45.08327747668339| 56.3163717815305|     0|\n",
      "|61.10666453684766|96.51142588489624|     1|\n",
      "|75.02474556738889|46.55401354116538|     1|\n",
      "|76.09878670226257|87.42056971926803|     1|\n",
      "|84.43281996120035|43.53339331072109|     1|\n",
      "+-----------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Create DataFrame Example\").getOrCreate()\n",
    "\n",
    "# Define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"score1\", DoubleType(), True),\n",
    "    StructField(\"score2\", DoubleType(), True),\n",
    "    StructField(\"result\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data to be loaded into the DataFrame\n",
    "data = [\n",
    "    (34.62365962451697, 78.0246928153624, 0),\n",
    "    (30.28671076822607, 43.89499752400101, 0),\n",
    "    (35.84740876993872, 72.90219802708364, 0),\n",
    "    (60.18259938620976, 86.30855209546826, 1),\n",
    "    (79.0327360507101, 75.3443764369103, 1),\n",
    "    (45.08327747668339, 56.3163717815305, 0),\n",
    "    (61.10666453684766, 96.51142588489624, 1),\n",
    "    (75.02474556738889, 46.55401354116538, 1),\n",
    "    (76.09878670226257, 87.42056971926803, 1),\n",
    "    (84.43281996120035, 43.53339331072109, 1)\n",
    "]\n",
    "\n",
    "# Create DataFrame with the specified schema\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame to verify its creation\n",
    "df.show()\n",
    "\n",
    "# Stop Spark session when done (not necessary if you will continue working)\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceaaa75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"score1\", \"score2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81b6e32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'marksDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/state/partition1/job-46463984/ipykernel_53157/4146107445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Transform your DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfeatureDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarksDf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Print the schema of the transformed DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'marksDf' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define your columns\n",
    "cols = [\"score1\", \"score2\"]\n",
    "\n",
    "# Create a new VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Transform your DataFrame\n",
    "featureDf = assembler.transform(marksDf)\n",
    "\n",
    "# Print the schema of the transformed DataFrame\n",
    "featureDf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34ed12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bdb635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fccfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
