{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77959b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def train_svm_validation_split(train_data, max_iter=100, reg_param=0.1):\n",
    "    \"\"\"\n",
    "    Trains an SVM model using the given training dataset.\n",
    "\n",
    "    Args:\n",
    "        train_data (pyspark.sql.DataFrame): The training dataset.\n",
    "        max_iter (int): Maximum number of iterations (default: 100).\n",
    "        reg_param (float): Regularization parameter (default: 0.1).\n",
    "\n",
    "    Returns:\n",
    "        pyspark.ml.classification.LinearSVCModel: The trained SVM model.\n",
    "    \"\"\"\n",
    "    svm = LinearSVC(labelCol=\"label\", featuresCol=\"features\", maxIter=max_iter, regParam=reg_param)\n",
    "    svm_model = svm.fit(train_data)\n",
    "    return svm_model\n",
    "\n",
    "def predict_svm_result(model, test_data):\n",
    "    \"\"\"\n",
    "    Makes predictions using the trained SVM model on the testing data.\n",
    "\n",
    "    Args:\n",
    "        model (pyspark.ml.classification.LinearSVCModel): The trained SVM model.\n",
    "        test_data (pyspark.sql.DataFrame): The testing data.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: The DataFrame with predicted results and actual labels.\n",
    "    \"\"\"\n",
    "    predictions = model.transform(test_data)\n",
    "    predictions = predictions.select(\"prediction\", \"label\")\n",
    "    return predictions\n",
    "\n",
    "def tenfold_svm_tuning_model(data):\n",
    "    \"\"\"\n",
    "    Performs tenfold cross-validation on the dataset and trains SVM models.\n",
    "\n",
    "    Args:\n",
    "        data (pyspark.sql.DataFrame): The original dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - model1_results: A list of tuples (iteration, sensitivity, specificity) for model 1.\n",
    "            - model2_results: A list of tuples (iteration, sensitivity, specificity) for model 2.\n",
    "    \"\"\"\n",
    "    # Separate the dataset into abnormal and normal people based on sy_glucose column\n",
    "    normal_people, abnormal_people = separate_datasets_by_glucose(data)\n",
    "\n",
    "    # Get the features of different models\n",
    "    feature1, feature2_1, feature2_2 = retrive_features()\n",
    "\n",
    "    # Initialize lists to store the results\n",
    "    model1_results = []\n",
    "    model2_results = []\n",
    "\n",
    "    # Perform tenfold cross-validation\n",
    "    for i in range(10):\n",
    "        # Split the abnormal and normal people datasets into train and test data\n",
    "        abnormal_train, abnormal_test = abnormal_people.randomSplit([0.9, 0.1], seed=i)\n",
    "        normal_train, normal_test = normal_people.randomSplit([0.9, 0.1], seed=i)\n",
    "\n",
    "        # Combine the train datasets using combine_train_datasets function\n",
    "        original_train_data = combine_train_datasets(abnormal_train, normal_train)\n",
    "\n",
    "        # Combine the test datasets using combine_testing_datasets function\n",
    "        main_test_data, dataset_number = combine_testing_datasets(normal_test, abnormal_test)\n",
    "\n",
    "        original_train_data = retrieve_selected_features(feature1, original_train_data)\n",
    "\n",
    "        # Train the original data (model 1)\n",
    "        model1 = train_svm_validation_split(original_train_data)\n",
    "\n",
    "        # Train model 2.1 and model 2.2\n",
    "        normal_train = retrieve_selected_features(feature2_1, normal_train)\n",
    "        model2_1 = train_svm_validation_split(normal_train)\n",
    "\n",
    "        abnormal_train = retrieve_selected_features(feature2_2, abnormal_train)\n",
    "        model2_2 = train_svm_validation_split(abnormal_train)\n",
    "\n",
    "        # Predict the results for model 1 for total people\n",
    "        test_data_model1 = retrieve_selected_features(feature1, main_test_data)\n",
    "        prediction1 = predict_svm_result(model1, test_data_model1)\n",
    "\n",
    "        # Calculate sensitivity and specificity for model 1\n",
    "        sensitivity1, specificity1 = calculate_sensitivity_specificity(prediction1)\n",
    "\n",
    "        # Save the results for model 1\n",
    "        model1_results.append((i+1, sensitivity1, specificity1))\n",
    "\n",
    "        # Predict the results for model 2.1 normal people and model 2.2 abnormal people\n",
    "        test_data_model2_1 = retrieve_selected_features(feature2_1, main_test_data)\n",
    "        prediction2_1 = predict_svm_result(model2_1, test_data_model2_1)\n",
    "\n",
    "        test_data_model2_2 = retrieve_selected_features(feature2_2, main_test_data)\n",
    "        prediction2_2 = predict_svm_result(model2_2, test_data_model2_2)\n",
    "\n",
    "        # Combine the predictions using combine_predictions function\n",
    "        prediction2 = combine_predictions(prediction2_1, prediction2_2, dataset_number)\n",
    "\n",
    "        # Calculate sensitivity and specificity for model 2\n",
    "        sensitivity2, specificity2 = calculate_sensitivity_specificity(prediction2)\n",
    "\n",
    "        # Save the results for model 2\n",
    "        model2_results.append((i+1, sensitivity2, specificity2))\n",
    "\n",
    "    return model1_results, model2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfdf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def train_random_forest_validation_split(train_data, max_depth=5, num_trees=100, seed=42):\n",
    "    \"\"\"\n",
    "    Trains a random forest model using the given training dataset.\n",
    "\n",
    "    Args:\n",
    "        train_data (pyspark.sql.DataFrame): The training dataset.\n",
    "        max_depth (int): Maximum depth of the tree (default: 5).\n",
    "        num_trees (int): Number of trees to train (default: 100).\n",
    "        seed (int): Random seed for reproducibility (default: 42).\n",
    "\n",
    "    Returns:\n",
    "        pyspark.ml.classification.RandomForestClassificationModel: The trained random forest model.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=max_depth, numTrees=num_trees, seed=seed)\n",
    "    rf_model = rf.fit(train_data)\n",
    "    return rf_model\n",
    "\n",
    "def predict_random_forest_result(model, test_data):\n",
    "    \"\"\"\n",
    "    Makes predictions using the trained random forest model on the testing data.\n",
    "\n",
    "    Args:\n",
    "        model (pyspark.ml.classification.RandomForestClassificationModel): The trained random forest model.\n",
    "        test_data (pyspark.sql.DataFrame): The testing data.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: The DataFrame with predicted results and actual labels.\n",
    "    \"\"\"\n",
    "    predictions = model.transform(test_data)\n",
    "    predictions = predictions.select(\"prediction\", \"label\")\n",
    "    return predictions\n",
    "\n",
    "def tenfold_random_forest_tuning_model(data):\n",
    "    \"\"\"\n",
    "    Performs tenfold cross-validation on the dataset and trains random forest models.\n",
    "\n",
    "    Args:\n",
    "        data (pyspark.sql.DataFrame): The original dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - model1_results: A list of tuples (iteration, sensitivity, specificity) for model 1.\n",
    "            - model2_results: A list of tuples (iteration, sensitivity, specificity) for model 2.\n",
    "    \"\"\"\n",
    "    # Separate the dataset into abnormal and normal people based on sy_glucose column\n",
    "    normal_people, abnormal_people = separate_datasets_by_glucose(data)\n",
    "\n",
    "    # Get the features of different models\n",
    "    feature1, feature2_1, feature2_2 = retrive_features()\n",
    "\n",
    "    # Initialize lists to store the results\n",
    "    model1_results = []\n",
    "    model2_results = []\n",
    "\n",
    "    # Perform tenfold cross-validation\n",
    "    for i in range(10):\n",
    "        # Split the abnormal and normal people datasets into train and test data\n",
    "        abnormal_train, abnormal_test = abnormal_people.randomSplit([0.9, 0.1], seed=i)\n",
    "        normal_train, normal_test = normal_people.randomSplit([0.9, 0.1], seed=i)\n",
    "\n",
    "        # Combine the train datasets using combine_train_datasets function\n",
    "        original_train_data = combine_train_datasets(abnormal_train, normal_train)\n",
    "\n",
    "        # Combine the test datasets using combine_testing_datasets function\n",
    "        main_test_data, dataset_number = combine_testing_datasets(normal_test, abnormal_test)\n",
    "\n",
    "        original_train_data = retrieve_selected_features(feature1, original_train_data)\n",
    "\n",
    "        # Train the original data (model 1)\n",
    "        model1 = train_random_forest_validation_split(original_train_data)\n",
    "\n",
    "        # Train model 2.1 and model 2.2\n",
    "        normal_train = retrieve_selected_features(feature2_1, normal_train)\n",
    "        model2_1 = train_random_forest_validation_split(normal_train)\n",
    "\n",
    "        abnormal_train = retrieve_selected_features(feature2_2, abnormal_train)\n",
    "        model2_2 = train_random_forest_validation_split(abnormal_train)\n",
    "\n",
    "        # Predict the results for model 1 for total people\n",
    "        test_data_model1 = retrieve_selected_features(feature1, main_test_data)\n",
    "        prediction1 = predict_random_forest_result(model1, test_data_model1)\n",
    "\n",
    "        # Calculate sensitivity and specificity for model 1\n",
    "        sensitivity1, specificity1 = calculate_sensitivity_specificity(prediction1)\n",
    "\n",
    "        # Save the results for model 1\n",
    "        model1_results.append((i+1, sensitivity1, specificity1))\n",
    "\n",
    "        # Predict the results for model 2.1 normal people and model 2.2 abnormal people\n",
    "        test_data_model2_1 = retrieve_selected_features(feature2_1, main_test_data)\n",
    "        prediction2_1 = predict_random_forest_result(model2_1, test_data_model2_1)\n",
    "\n",
    "        test_data_model2_2 = retrieve_selected_features(feature2_2, main_test_data)\n",
    "        prediction2_2 = predict_random_forest_result(model2_2, test_data_model2_2)\n",
    "\n",
    "        # Combine the predictions using combine_predictions function\n",
    "        prediction2 = combine_predictions(prediction2_1, prediction2_2, dataset_number)\n",
    "\n",
    "        # Calculate sensitivity and specificity for model 2\n",
    "        sensitivity2, specificity2 = calculate_sensitivity_specificity(prediction2)\n",
    "\n",
    "        # Save the results for model 2\n",
    "        model2_results.append((i+1, sensitivity2, specificity2))\n",
    "\n",
    "    return model1_results, model2_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
